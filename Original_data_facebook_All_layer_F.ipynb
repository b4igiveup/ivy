{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b4igiveup/ivy/blob/master/Original_data_facebook_All_layer_F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nk51civsKZG",
        "outputId": "87d2f5d0-f406-45e9-9113-cb13c9b06ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.7/dist-packages (1.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dPno6JF3oiV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import esm\n",
        "import pandas as pd\n",
        "\n",
        "layers=[0,15,33]\n",
        "# Load ESM-1b model\n",
        "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
        "batch_converter = alphabet.get_batch_converter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl0KUzeX3oi0",
        "outputId": "e928faa5-f777-4355-cacc-fcbbb1090d03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DP01096\n",
            "Fasta length:  2924\n",
            "No. of Chunks:  3\n",
            "chunk length:  1022\n"
          ]
        }
      ],
      "source": [
        "import pathlib \n",
        "import numpy as np\n",
        "workspace2=\"test\"\n",
        "np.set_printoptions(precision=3)\n",
        "workspace=\"./\"+workspace2\n",
        "pathlib.Path(workspace+\"Features\").mkdir(parents=True, exist_ok=True) \n",
        "\n",
        "df_merge=pd.DataFrame()\n",
        "with open(workspace2+\"_fasta.txt\") as fasta_file:\n",
        "    \n",
        "    flagpid=0\n",
        "    for pid in fasta_file:\n",
        "\n",
        "        pid=pid[1:-1]\n",
        "        pid=pid.strip()\n",
        "        fastseq=fasta_file.readline()\n",
        "        print(pid)\n",
        "        # print(fastseq[:-1])\n",
        "        fasta=fastseq[:-1]\n",
        "\n",
        "        print(\"Fasta length: \", len(fasta))\n",
        "\n",
        "        if(len(fasta)>1022):\n",
        "                n = 1022\n",
        "                chunks = [fasta[i:i+n] for i in range(0, len(fasta), n)]\n",
        "                print(\"No. of Chunks: \",len(chunks))\n",
        "\n",
        "                flagc=0\n",
        "                for chunk in chunks:\n",
        "                    print(\"chunk length: \",len(chunk))\n",
        "                    # print(chunk)\n",
        "\n",
        "                    data = [ (pid, chunk)]\n",
        "                    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "                    flagl=0\n",
        "                    # print(\"Layer\", layern)\n",
        "                    layern=list(range(34)) \n",
        "                    with torch.no_grad():\n",
        "                        results = model(batch_tokens, repr_layers=layern, return_contacts=True)\n",
        "                    # Extract per-residue representations (on CPU)\n",
        "                    for layern in range(34): #\n",
        "                                                                     \n",
        "                        token_representations = results[\"representations\"][layern]\n",
        "                        toekn=token_representations[0, 1 : len(chunk) + 1].numpy()\n",
        "                        toekn=np.hstack((toekn,np.mean(toekn, axis=1).reshape(-1,1)))\n",
        "                        if(flagl==0):\n",
        "                            np_featurel=toekn\n",
        "                            flagl=1\n",
        "                        else:\n",
        "                            np_featurel=np.hstack((np_featurel,toekn))                       \n",
        "      \n",
        "\n",
        "                if(flagc==0):\n",
        "                    np_featurell=np_featurel\n",
        "                    flagc=1\n",
        "                else:\n",
        "                    np_featurell=np.vstack((np_featurell,np_featurel))\n",
        "                # print(np_featurell.shape)\n",
        "\n",
        "        else:\n",
        "\n",
        "            data = [ (pid, fasta)]\n",
        "            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "            flagl=0\n",
        "            layern=list(range(34))    \n",
        "            # print(\"Layer\", layern)\n",
        "            # Extract per-residue representations (on CPU)\n",
        "            with torch.no_grad():\n",
        "                results = model(batch_tokens, repr_layers=layern, return_contacts=True)\n",
        "     \n",
        "                  \n",
        "            for layern in range(34): #               \n",
        "                \n",
        "                token_representations = results[\"representations\"][layern]\n",
        "                toekn=token_representations[0, 1 : len(fasta) + 1].numpy()\n",
        "                toekn=np.hstack((toekn,np.mean(toekn, axis=1).reshape(-1,1)))\n",
        "\n",
        "                if(flagl==0):\n",
        "                    np_featurell=toekn\n",
        "                    flagl=1\n",
        "                else:\n",
        "                    np_featurell=np.hstack((np_featurell,toekn))                  \n",
        "     \n",
        "        \n",
        "        if(flagpid==0):\n",
        "            np_feature=np_featurell\n",
        "            flagpid=1\n",
        "        else:\n",
        "            np_feature=np.vstack((np_feature,np_featurell))\n",
        "        print(np_feature.shape) \n",
        "\n",
        "df_features=pd.DataFrame(np_feature)\n",
        "df_features.columns= df_features.columns.astype(str)+\"_FlModel\"      \n",
        "df_features.to_feather(workspace+\"Features/\"+pid+\".ft\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Mfh50Q3ojE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': poetry)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "a67c9df62a9301a0e61bce5ce0410148aa0769c8a29eb63f8d365773b2f01d9e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}